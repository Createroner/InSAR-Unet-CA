import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F  # <--- NEW: Import torch.nn.functional
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T
import torchvision.transforms.functional as F_T 
import torchvision.models.segmentation as segmentation 
import numpy as np
from PIL import Image
import os
import time
import json 
from typing import Tuple, List, Dict, Any
import numpy as np

# --- é…ç½®å‚æ•° (Configuration Parameters) ---

# TODO: ã€é‡è¦ã€‘è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬åç”Ÿæˆçš„ 'VOCdevkit/VOC2012' ç›®å½•çš„å®é™…ä½ç½®ã€‚
VOC_ROOT = "../testsize647500" 

# è¾“å…¥å›¾åƒçš„å°ºå¯¸
IMAGE_SIZE = 64
# åˆ†å‰²ç±»åˆ«æ•° (èƒŒæ™¯ 0 + æµ·å†° 1)
NUM_CLASSES = 2 
# è®­ç»ƒå‚æ•°
BATCH_SIZE = 128
NUM_EPOCHS = 100
LEARNING_RATE = 1e-4

# TODO: ã€é‡è¦ã€‘æ¨¡å‹ä¿å­˜è·¯å¾„ã€‚
MODEL_SAVE_PATH = "trained_models/fcn_sea_ice_segmentation_64x64_spatial_attn_best.pth" 
# ã€æ–°å¢ã€‘æŒ‡æ ‡ä¿å­˜è·¯å¾„
METRICS_SAVE_PATH = "training_metrics/training_history_fcn_64x64_spatial_attn.json"

# ã€ä¿®æ”¹ã€‘æŒ‡å®šä½¿ç”¨çš„ GPU ID (ä¾‹å¦‚ GPU 0)
GPU_ID = 4
if torch.cuda.is_available():
    try:
        torch.cuda.set_device(GPU_ID)
        DEVICE = torch.device(f"cuda:{GPU_ID}")
    except RuntimeError:
        print(f"è­¦å‘Š: GPU ID {GPU_ID} ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU (cuda:0) æˆ– CPUã€‚")
        DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
else:
    DEVICE = torch.device("cpu")
print(f"Using device: {DEVICE}")

# --- 1. ç©ºé—´æ³¨æ„åŠ›æ¨¡å— (Spatial Attention Module) ---

class SpatialAttention(nn.Module):
    """
    ç®€åŒ–çš„ç©ºé—´æ³¨æ„åŠ›æ¨¡å— (Inspired by CBAM spatial attention)
    è¾“å…¥ç‰¹å¾å›¾ X -> å·ç§¯å¾—åˆ°æ³¨æ„åŠ›å›¾ M -> X * M
    """
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        
        x_concat = torch.cat([avg_out, max_out], dim=1)
        
        attention_map = self.conv(x_concat)
        attention_weights = self.sigmoid(attention_map)
        
        return x * attention_weights

# --- 2. FCN æ¨¡å‹å®šä¹‰ (FCN Model Definition) ---

class FCN_SingleChannel_SpatialAttn(nn.Module):
    """
    å°è£… torchvision FCN æ¨¡å‹ï¼Œæ”¯æŒå•é€šé“è¾“å…¥ï¼Œé€‚é… NUM_CLASSESï¼Œå¹¶æ·»åŠ ç©ºé—´æ³¨æ„åŠ›ã€‚
    """
    def __init__(self, num_classes: int = 2, backbone: str = 'resnet50', pretrained: bool = False):
        super(FCN_SingleChannel_SpatialAttn, self).__init__()
        
        if backbone == 'resnet50':
            self.model = segmentation.fcn_resnet50(pretrained=pretrained, progress=True)
            LOW_LEVEL_IN_CHANNELS = 2048
        elif backbone == 'resnet101':
            self.model = segmentation.fcn_resnet101(pretrained=pretrained, progress=True)
            LOW_LEVEL_IN_CHANNELS = 2048
        else:
            raise ValueError(f"Unsupported backbone: {backbone}")

        # --- æ ¸å¿ƒä¿®æ”¹ 1: é€‚é…å•é€šé“è¾“å…¥ ---
        original_conv1 = self.model.backbone.conv1 
        new_conv1 = nn.Conv2d(
            1, # è¾“å…¥é€šé“æ”¹ä¸º 1
            original_conv1.out_channels,
            kernel_size=original_conv1.kernel_size,
            stride=original_conv1.stride,
            padding=original_conv1.padding,
            bias=original_conv1.bias is not None
        )
        # æƒé‡è¿ç§»é€»è¾‘
        if pretrained:
            with torch.no_grad():
                mean_weights = original_conv1.weight.mean(dim=1, keepdim=True)
                new_conv1.weight.data = mean_weights
                if original_conv1.bias is not None:
                    new_conv1.bias.data = original_conv1.bias.data
        self.model.backbone.conv1 = new_conv1
        
        # --- æ ¸å¿ƒä¿®æ”¹ 2: æ›¿æ¢æ•´ä¸ªåˆ†ç±»å™¨æ¨¡å—ä»¥é€‚é…ç±»åˆ«æ•° (NUM_CLASSES) ---
        new_classifier = segmentation.fcn.FCNHead(LOW_LEVEL_IN_CHANNELS, num_classes)
        self.model.classifier = new_classifier
        
        # ğŸš¨ ã€æ–°å¢ã€‘ç©ºé—´æ³¨æ„åŠ›æ¨¡å— 
        self.spatial_attention = SpatialAttention() 
        
        conv_output_layer = self.model.classifier[3]
        if isinstance(conv_output_layer, nn.Conv2d):
            print(f"Model configured: Conv1 in_channels={self.model.backbone.conv1.in_channels}, Classifier out_channels={conv_output_layer.out_channels}")
        else:
            print("Model configured. Output layer type check pending (Expected Conv2d).")


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ğŸš¨ ã€è¾“å…¥æ£€æŸ¥ã€‘
        if x.dim() != 4 or x.size(1) != 1:
            raise ValueError(f"FCN_SingleChannel_SpatialAttn expected input shape (B, 1, H, W), but got {x.shape}. Check DataLoader/Dataset.")
        
        input_shape = x.shape[-2:]
        
        # 1. è·å– ResNet Backbone çš„è¾“å‡º
        features = self.model.backbone(x) 
        
        # 2. ä»ç‰¹å¾å­—å…¸ä¸­è·å–ä¸»è¦ç‰¹å¾
        high_level_feature = features['out'] 
        
        # 3. ğŸš¨ ã€åº”ç”¨ç©ºé—´æ³¨æ„åŠ›ã€‘
        attended_high_level_feature = self.spatial_attention(high_level_feature)
        
        # 4. å°†æ³¨æ„åŠ›åçš„ç‰¹å¾æ”¾å›ç‰¹å¾å­—å…¸
        features['out'] = attended_high_level_feature
        
        # 5. å°†ç‰¹å¾å­—å…¸ä¼ å…¥ FCN çš„åˆ†ç±»å™¨
        x = self.model.classifier(features['out'])
        
        # 6. æœ€ç»ˆä¸Šé‡‡æ ·åˆ°è¾“å…¥å°ºå¯¸
        # ä¿®æ­£: ä½¿ç”¨ F.interpolateï¼Œå®ƒæ”¯æŒ mode (bilinear) å’Œ align_cornersã€‚
        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False) 
        
        return x

# --- 3. æ•°æ®é›†åŠ è½½ (Dataset Loader) ---
class VOCSegDataset(Dataset):
    """åŠ è½½ç”±é¢„å¤„ç†è„šæœ¬ç”Ÿæˆçš„ VOC æ ¼å¼æµ·å†°åˆ†å‰²æ•°æ®é›†ã€‚"""
    def __init__(self, voc_root: str, image_size: int, image_set: str = 'train', transforms=None):
        self.voc_root = voc_root
        self.image_size = image_size
        self.transforms = transforms
        
        self.image_dir = os.path.join(voc_root, 'JPEGImages')
        self.mask_dir = os.path.join(voc_root, 'SegmentationClass')
        self.image_set_path = os.path.join(voc_root, 'ImageSets', 'Segmentation', f'{image_set}.txt')

        if not os.path.exists(self.image_set_path):
            raise FileNotFoundError(f"ImageSets æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·ç¡®è®¤æ–‡ä»¶ {image_set}.txt å­˜åœ¨ã€‚Path: {self.image_set_path}")

        with open(self.image_set_path, 'r') as f:
            self.ids = [line.strip() for line in f.readlines()]
            
        print(f"æˆåŠŸåŠ è½½ {image_set} é›†åˆçš„ {len(self.ids)} ä¸ªåˆ‡ç‰‡æ•°æ®ã€‚")

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        img_id = self.ids[idx]
        
        img_path = os.path.join(self.image_dir, f"{img_id}.jpg")
        img = Image.open(img_path).convert('L') # è¯»å–ä¸ºå•é€šé“ç°åº¦å›¾ 'L'
        
        mask_path = os.path.join(self.mask_dir, f"{img_id}.png")
        mask = Image.open(mask_path).convert('L') # è¯»å–æ©è†œï¼Œè½¬ä¸ºå•é€šé“
        
        if self.transforms is not None:
            img = self.transforms(img)

        # æ©è†œ Resize ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
        mask = F_T.resize(mask, 
                             (self.image_size, self.image_size), 
                             interpolation=T.InterpolationMode.NEAREST)
        
        mask = T.ToTensor()(mask)
        # ç¡®ä¿æ©è†œæ˜¯ LongTensor ä¸”å½¢çŠ¶ä¸º (H, W)
        mask = mask.squeeze(0).long() 

        return img, mask

# --- 4. æ€§èƒ½æŒ‡æ ‡è®¡ç®—è¾…åŠ©å‡½æ•° (Metrics Computation) ---
def compute_metrics(outputs: torch.Tensor, masks: torch.Tensor, num_classes: int) -> Dict[str, float]:
    """è®¡ç®— PA/OA, mIoU, mPA å’Œ mF1-Scoreã€‚"""
    _, preds = torch.max(outputs, 1)
    
    valid_mask = (masks != 255)
    
    preds_flat = preds[valid_mask].cpu().numpy()
    masks_flat = masks[valid_mask].cpu().numpy()
    
    TP = np.zeros(num_classes)
    FP = np.zeros(num_classes)
    FN = np.zeros(num_classes)

    for cls in range(num_classes):
        TP[cls] = ((masks_flat == cls) & (preds_flat == cls)).sum()
        FP[cls] = ((masks_flat != cls) & (preds_flat == cls)).sum()
        FN[cls] = ((masks_flat == cls) & (preds_flat != cls)).sum()

    # 1. Overall Accuracy (OA)
    total_correct_pixels = TP.sum() 
    total_pixels = TP.sum() + FP.sum() + FN.sum() 
    acc = total_correct_pixels / total_pixels if total_pixels > 0 else 0.0

    # 2. Mean IoU (mIoU)
    union = TP + FP + FN
    iou = np.divide(TP, union, out=np.zeros_like(TP, dtype=float), where=union!=0)
    miou = np.mean(iou[union > 0]) if np.any(union > 0) else 0.0

    # 3. Mean Pixel Accuracy (mPA)
    recall = np.divide(TP, (TP + FN), out=np.zeros_like(TP, dtype=float), where=(TP + FN)!=0)
    mpa = np.mean(recall[(TP + FN) > 0]) if np.any((TP + FN) > 0) else 0.0

    # 4. Mean F1-Score (mF1)
    precision = np.divide(TP, (TP + FP), out=np.zeros_like(TP, dtype=float), where=(TP + FP)!=0)
    f1_scores = np.divide(2 * precision * recall, (precision + recall), 
                             out=np.zeros_like(TP, dtype=float), 
                             where=(precision + recall)!=0)
    mf1 = np.mean(f1_scores[(TP + FN) > 0]) if np.any((TP + FN) > 0) else 0.0
    
    return {
        'acc': acc, 
        'miou': miou,
        'mpa': mpa,
        'mf1': mf1
    }

# --- 5. éªŒè¯è¾…åŠ©å‡½æ•° (Validation Function) ---
@torch.no_grad()
def validate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.Module) -> Dict[str, float]:
    """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿”å›æŒ‡æ ‡å­—å…¸"""
    model.eval()
    running_loss = 0.0
    total_metrics = {'acc': 0.0, 'miou': 0.0, 'mpa': 0.0, 'mf1': 0.0}
    num_samples = 0

    for images, masks in dataloader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)
        
        outputs = model(images)
        loss = criterion(outputs, masks)
        
        running_loss += loss.item() * images.size(0)
        
        metrics = compute_metrics(outputs.detach(), masks.detach(), NUM_CLASSES)
        
        for key in total_metrics:
            total_metrics[key] += metrics[key] * images.size(0)
            
        num_samples += images.size(0)

    if num_samples > 0:
        val_metrics = {
            'val_loss': running_loss / num_samples,
            'val_acc': total_metrics['acc'] / num_samples,
            'val_miou': total_metrics['miou'] / num_samples,
            'val_mpa': total_metrics['mpa'] / num_samples,
            'val_mf1': total_metrics['mf1'] / num_samples,
        }
    else:
        val_metrics = {'val_loss': 0.0, 'val_acc': 0.0, 'val_miou': 0.0, 'val_mpa': 0.0, 'val_mf1': 0.0}

    print(f"\n--- éªŒè¯é›†è¯„ä¼°ç»“æœ ---")
    print(f"éªŒè¯é›†å¹³å‡ Loss: {val_metrics['val_loss']:.4f}")
    print(f"éªŒè¯é›†å¹³å‡ mIoU: {val_metrics['val_miou']:.4f}")
    
    model.train()
    return val_metrics

# --- 6. è®­ç»ƒä¸»å‡½æ•° (Main Training Function) ---
def train_model(model: nn.Module, train_dataloader: DataLoader, val_dataloader: DataLoader, 
                 criterion: nn.Module, optimizer: optim.Optimizer, num_epochs: int = NUM_EPOCHS) -> List[Dict[str, Any]]:
    """æ‰§è¡Œè®­ç»ƒå¾ªç¯ï¼Œå¹¶åœ¨è®­ç»ƒç»“æŸåä¿å­˜è®­ç»ƒå†å²è®°å½•"""
    model.to(DEVICE)
    
    start_time = time.time()
    best_m_iou = -1.0 
    history: List[Dict[str, Any]] = [] 
    
    for epoch in range(num_epochs):
        # --- è®­ç»ƒé˜¶æ®µ ---
        model.train()
        running_loss = 0.0
        total_metrics = {'acc': 0.0, 'miou': 0.0, 'mpa': 0.0, 'mf1': 0.0}
        
        for i, (images, masks) in enumerate(train_dataloader):
            images = images.to(DEVICE)
            masks = masks.to(DEVICE) 
            
            if images.dim() != 4 or images.size(1) != 1:
                raise RuntimeError(f"FATAL ERROR: Input image tensor shape mismatch BEFORE model call. Expected (B, 1, H, W), but got {images.shape}. Check DataLoader/Dataset.")

            optimizer.zero_grad()
            outputs = model(images) 
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            
            metrics = compute_metrics(outputs.detach(), masks.detach(), NUM_CLASSES)
            
            for key in total_metrics:
                total_metrics[key] += metrics[key] * images.size(0)

            if (i + 1) % 100 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Train Loss: {loss.item():.4f}, Train mIoU: {metrics['miou']:.4f}")

        # --- Epoch ç»“æŸæ—¶æ€»ç»“ (è®­ç»ƒé›†) ---
        num_train_samples = len(train_dataloader.dataset)
        train_metrics = {
            'epoch': epoch + 1,
            'train_loss': running_loss / num_train_samples,
            'train_acc': total_metrics['acc'] / num_train_samples,
            'train_miou': total_metrics['miou'] / num_train_samples,
            'train_mpa': total_metrics['mpa'] / num_train_samples,
            'train_mf1': total_metrics['mf1'] / num_train_samples,
        }
        
        print(f"\n--- Epoch {epoch+1}/{num_epochs} å®Œæˆ (è®­ç»ƒé›†) ---")
        print(f"è®­ç»ƒé›†æ€»å¹³å‡ Loss: {train_metrics['train_loss']:.4f}")
        print(f"è®­ç»ƒé›†æ€»å¹³å‡ mIoU: {train_metrics['train_miou']:.4f}")
        
        # --- éªŒè¯é˜¶æ®µ ---
        if val_dataloader:
            val_metrics = validate_model(model, val_dataloader, criterion)
            current_m_iou = val_metrics['val_miou']
            
            epoch_history = {**train_metrics, **val_metrics} 
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºéªŒè¯é›† mIoU)
            if current_m_iou > best_m_iou:
                best_m_iou = current_m_iou
                os.makedirs(os.path.dirname(MODEL_SAVE_PATH) or '.', exist_ok=True)
                torch.save(model.state_dict(), MODEL_SAVE_PATH) 
                print(f"*** Val mIoU æå‡è‡³ {best_m_iou:.4f}ï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH} ***")
            else:
                print(f"Val mIoU ({current_m_iou:.4f}) æœªè¶…è¿‡æœ€ä½³ mIoU ({best_m_iou:.4f}).")
        else:
            epoch_history = train_metrics

        history.append(epoch_history)


    end_time = time.time()
    print(f"\nè®­ç»ƒæµç¨‹å®Œæˆ! æ€»è€—æ—¶: {(end_time - start_time) / 60:.2f} åˆ†é’Ÿ")
    
    return history 

# --- 7. ä¸»å‡½æ•° (Main Execution) ---

def main():
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.isdir(os.path.join(VOC_ROOT, 'JPEGImages')):
        print("é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½•ã€‚è¯·ç¡®ä¿æ‚¨å·²è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼Œå¹¶å°† VOC_ROOT è®¾ç½®æ­£ç¡®ã€‚")
        print(f"æœŸæœ›çš„ç›®å½•: {VOC_ROOT}")
        return

    # --- æ•°æ®å¢å¼ºä¸å½’ä¸€åŒ– ---
    data_transforms = T.Compose([
        T.Resize((IMAGE_SIZE, IMAGE_SIZE)), 
        T.ToTensor(),
        T.Normalize(mean=[0.5], std=[0.5]) 
    ])
    
    # --- å®ä¾‹åŒ–æ•°æ®é›†å’Œ DataLoader ---
    num_workers = os.cpu_count() // 2 if os.cpu_count() else 2
    
    try:
        train_dataset = VOCSegDataset(
            voc_root=VOC_ROOT, image_size=IMAGE_SIZE, image_set='train', transforms=data_transforms
        )
        train_dataloader = DataLoader(
            train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=(DEVICE.type == 'cuda')
        )
        
        val_dataset = VOCSegDataset(
            voc_root=VOC_ROOT, image_size=IMAGE_SIZE, image_set='val', transforms=data_transforms
        )
        val_dataloader = DataLoader(
            val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=(DEVICE.type == 'cuda')
        )

    except FileNotFoundError as e:
        print(e)
        return
        
    if len(train_dataset) == 0:
        print("è®­ç»ƒæ•°æ®é›†ä¸ºç©ºã€‚")
        return

    # --- å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    print("æ­£åœ¨å®ä¾‹åŒ– FCN æ¨¡å‹ (Backbone: ResNet50)ï¼Œå¹¶é›†æˆç©ºé—´æ³¨æ„åŠ›ã€‚")
    model = FCN_SingleChannel_SpatialAttn(num_classes=NUM_CLASSES, backbone='resnet50', pretrained=False) 
    
    criterion = nn.CrossEntropyLoss(ignore_index=255)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # --- å¼€å§‹è®­ç»ƒå¹¶è·å–å†å²è®°å½• ---
    training_history = train_model(model, train_dataloader, val_dataloader, criterion, optimizer)
    
    # --- ä¿å­˜è®­ç»ƒå†å²è®°å½•åˆ° JSON æ–‡ä»¶ ---
    if training_history:
        os.makedirs(os.path.dirname(METRICS_SAVE_PATH) or '.', exist_ok=True)
        try:
            with open(METRICS_SAVE_PATH, 'w') as f:
                json_history = [{k: (v if not isinstance(v, (torch.Tensor, np.floating)) else float(v)) for k, v in epoch_data.items()} 
                                for epoch_data in training_history]
                json.dump(json_history, f, indent=4)
            print(f"\nâœ… è®­ç»ƒå†å²æŒ‡æ ‡å·²ä¿å­˜è‡³: {METRICS_SAVE_PATH}")
            
        except Exception as e:
            print(f"âŒ è­¦å‘Šï¼šä¿å­˜æŒ‡æ ‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main()